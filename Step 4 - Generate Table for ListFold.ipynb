{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2527ea5-a381-483d-ab3c-7e067f5650dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import datetime\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from tqdm import tqdm\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Optional: Only import catboost if you plan to use it. \n",
    "# Otherwise, you can lazily import it inside the class below.\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "catboost_params = {\n",
    "    'iterations': 1000,\n",
    "    'depth': 10,\n",
    "    'border_count': 128,\n",
    "    'boosting_type': 'Ordered',\n",
    "    'task_type': 'GPU',\n",
    "    'devices': '0',\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'bagging_temperature': 0.5,\n",
    "    'learning_rate': 0.03,\n",
    "    'l2_leaf_reg': 1,\n",
    "    'scale_pos_weight': 2.964,\n",
    "    'eval_metric': 'F1',   # Typically for classification; adjust if you're doing regression\n",
    "    'random_seed': 42,\n",
    "    'verbose': 0,\n",
    "    'use_best_model': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9ab603-8885-40d5-9842-a1f8af0e0b77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8620b6ba-951c-4671-b835-7d45682d0566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b123bd-43a6-4ae7-83f4-2e8f0339d8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98a03b-3075-4bd7-821c-10d8d1e6bb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFetcher:\n",
    "    \"\"\"\n",
    "    A helper class to interact with BigQuery and fetch the necessary data for the pipeline.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 project_id: str = 'issachar-feature-library', \n",
    "                 dataset_name: str = 'wmg'):\n",
    "        \"\"\"\n",
    "        Initialize the DataFetcher with BigQuery connection details.\n",
    "        \n",
    "        Args:\n",
    "            project_id (str): Google Cloud project ID.\n",
    "            dataset_name (str): BigQuery dataset name.\n",
    "        \"\"\"\n",
    "        self.project_id = project_id\n",
    "        self.dataset_name = dataset_name\n",
    "        self.client = bigquery.Client(project=self.project_id)\n",
    "    \n",
    "    def get_unique_signals_from_daily_ic(self, table_name: str = 'ic_daily2'):\n",
    "        \"\"\"\n",
    "        Fetch all unique signal values from the 'signal' column of the daily_ic table.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): BigQuery table name containing the daily IC results.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of unique signal names.\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT DISTINCT signal\n",
    "        FROM `{self.project_id}.{self.dataset_name}.{table_name}`\n",
    "        \"\"\"\n",
    "        query_job = self.client.query(query)\n",
    "        df = query_job.to_dataframe()\n",
    "        return df['signal'].unique().tolist()\n",
    "\n",
    "    def fetch_data_from_gbq(self, table_name: str):\n",
    "        \"\"\"\n",
    "        Fetches all rows & columns from a specified BigQuery table.\n",
    "        Converts the 'date' column to datetime and sets a MultiIndex of (date, signal).\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): The BigQuery table name.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with a MultiIndex of (date, signal).\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{self.project_id}.{self.dataset_name}.{table_name}`\n",
    "        \"\"\"\n",
    "        query_job = self.client.query(query)\n",
    "        df = query_job.to_dataframe()\n",
    "\n",
    "        # Convert date to datetime, drop timezone info\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "\n",
    "        # Make (date, signal) a MultiIndex if both columns exist\n",
    "        if 'signal' in df.columns and 'date' in df.columns:\n",
    "            df.set_index(['date', 'signal'], inplace=True)\n",
    "\n",
    "        return df.sort_index()\n",
    "    \n",
    "    def fetch_data_from_gbq_fvalues(self, \n",
    "                                   table_name: str, \n",
    "                                   signals: list):\n",
    "        \"\"\"\n",
    "        Fetches data from a BigQuery table and filters rows to include only those\n",
    "        where the \"signal\" column matches one of the provided signals.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): The BigQuery table name.\n",
    "            signals (list): A list of signal values to filter the rows.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing the filtered rows.\n",
    "        \"\"\"\n",
    "        # Prepare the signals list for SQL IN clause by quoting each signal\n",
    "        signals_str = \", \".join(f\"'{signal}'\" for signal in signals)\n",
    "\n",
    "        query = f\"\"\"\n",
    "        SELECT date, signal, \n",
    "               fvalue, \n",
    "               fvalue_median_21_minus_fvalue_median_252,\n",
    "               fvalue_median_21\n",
    "        FROM `{self.project_id}.{self.dataset_name}.{table_name}`\n",
    "        WHERE signal IN ({signals_str})\n",
    "        \"\"\"\n",
    "        query_job = self.client.query(query)\n",
    "        df = query_job.to_dataframe()\n",
    "\n",
    "        # Convert date to datetime, drop timezone info\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "        df.set_index(['date', 'signal'], inplace=True)\n",
    "\n",
    "        return df.sort_index()\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \"\"\"\n",
    "    A helper class to engineer features from the fetched data.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def compute_rolling_ic(df_ic_daily: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Append rolling IC metrics (21-day rolling mean) to the daily IC DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df_ic_daily (pd.DataFrame): Input DataFrame of daily IC, indexed by (date, signal).\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: The updated DataFrame with new rolling columns.\n",
    "        \"\"\"\n",
    "        # 21-day rolling for all_IC_spearmanr\n",
    "        daily_rolling_ic = df_ic_daily.groupby('signal')['all_IC_spearmanr'].rolling(21).mean()\n",
    "        daily_rolling_ic.index = daily_rolling_ic.index.droplevel(0)\n",
    "\n",
    "        # 21-day rolling for topq_IC_spearmanr\n",
    "        daily_rolling_ic_top = df_ic_daily.groupby('signal')['topq_IC_spearmanr'].rolling(21).mean()\n",
    "        daily_rolling_ic_top.index = daily_rolling_ic_top.index.droplevel(0)\n",
    "\n",
    "        df_ic_daily['all_IC_spearmanr_rolling_21'] = daily_rolling_ic\n",
    "        df_ic_daily['topq_IC_spearmanr_rolling_21'] = daily_rolling_ic_top\n",
    "        \n",
    "        # Diff feature\n",
    "        df_ic_daily['rolling_diff'] = daily_rolling_ic - daily_rolling_ic_top\n",
    "        \n",
    "        return df_ic_daily\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_rolling_features(group: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute various rolling transformations (averages, vol, squared terms, etc.)\n",
    "        on a per-signal basis.\n",
    "\n",
    "        Args:\n",
    "            group (pd.DataFrame): Subset of DataFrame belonging to one signal.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The group with additional rolling feature columns.\n",
    "        \"\"\"\n",
    "        group = group.sort_index()\n",
    "        \n",
    "        # Rolling means of all_IC_spearmanr\n",
    "        group['IC_all_5'] = group['all_IC_spearmanr'].rolling(window=5, min_periods=1).mean()\n",
    "        group['IC_all_20'] = group['all_IC_spearmanr'].rolling(window=20, min_periods=1).mean()\n",
    "        group['IC_all_5_diff_IC_all_20'] = group['IC_all_5'] - group['IC_all_20']\n",
    "        \n",
    "        # Rolling means of topq_IC_spearmanr\n",
    "        group['IC_top_5'] = group['topq_IC_spearmanr'].rolling(window=5, min_periods=1).mean()\n",
    "        group['IC_top_20'] = group['topq_IC_spearmanr'].rolling(window=20, min_periods=1).mean()\n",
    "        group['IC_top_5_diff_IC_top_20'] = group['IC_top_5'] - group['IC_top_20']\n",
    "        \n",
    "        # Rolling volatility (std)\n",
    "        group['IC_all_vol_5'] = group['all_IC_spearmanr'].rolling(window=5, min_periods=1).std()\n",
    "        group['IC_all_vol_20'] = group['all_IC_spearmanr'].rolling(window=20, min_periods=1).std()\n",
    "        group['IC_all_vol_5_diff_IC_all_vol_20'] = group['IC_all_vol_5'] - group['IC_all_vol_20']\n",
    "        \n",
    "        group['IC_top_vol_5'] = group['topq_IC_spearmanr'].rolling(window=5, min_periods=1).std()\n",
    "        group['IC_top_vol_20'] = group['topq_IC_spearmanr'].rolling(window=20, min_periods=1).std()\n",
    "        group['IC_top_vol_5_diff_IC_top_vol_20'] = group['IC_top_vol_5'] - group['IC_top_vol_20']\n",
    "        \n",
    "        # Squared terms\n",
    "        group['IC_all_sq'] = group['all_IC_spearmanr'] ** 2\n",
    "        group['IC_top_sq'] = group['topq_IC_spearmanr'] ** 2\n",
    "        group['IC_all_sq_diff_IC_top_sq'] = group['IC_all_sq'] - group['IC_top_sq']\n",
    "        \n",
    "        group['IC_all_vol_5_sq'] = group['IC_all_vol_5'] ** 2\n",
    "        group['IC_top_vol_5_sq'] = group['IC_top_vol_5'] ** 2\n",
    "        group['IC_all_vol_5_sq_minus_IC_top_vol_5_sq'] = group['IC_all_vol_5_sq'] - group['IC_top_vol_5_sq']\n",
    "        \n",
    "        # Log transform of 5-day vol (small constant to avoid log(0))\n",
    "        group['log_IC_all_vol_5'] = np.log(group['IC_all_vol_5_sq'] + 1e-8)\n",
    "        group['log_IC_top_vol_5'] = np.log(group['IC_top_vol_5_sq'] + 1e-8)\n",
    "        group['log_IC_all_vol_5_diff_log_IC_top_vol_5'] = group['log_IC_all_vol_5'] - group['log_IC_top_vol_5']\n",
    "        \n",
    "        # Interaction: product of 5-day rolling average and 5-day volatility\n",
    "        group['IC_5_x_vol_5_all'] = group['IC_all_5'] * group['IC_all_vol_5']\n",
    "        group['IC_20_x_vol_20_all'] = group['IC_all_20'] * group['IC_all_vol_20']\n",
    "        group['IC_5_x_vol_5_top'] = group['IC_top_5'] * group['IC_top_vol_5']\n",
    "        group['IC_20_x_vol_20_top'] = group['IC_top_20'] * group['IC_top_vol_20']\n",
    "        \n",
    "        # Volatility-adjusted IC\n",
    "        group['IC_all_vol_adj'] = group['IC_all_5'] / (group['IC_all_vol_5'] + 1e-8)\n",
    "        group['IC_top_vol_adj'] = group['IC_top_5'] / (group['IC_top_vol_5'] + 1e-8)\n",
    "        \n",
    "        # Trend indicators\n",
    "        group['IC_all_trend'] = group['IC_all_5'] - group['IC_all_20']\n",
    "        group['IC_top_trend'] = group['IC_top_5'] - group['IC_top_20']\n",
    "\n",
    "        # Rate of change of monotonicity\n",
    "        group['monotonicity_21_diff_accel_5'] = group['monotonic_21_diff'].pct_change().rolling(5).sum()\n",
    "        group['monotonic_increasing_pct_21_accel_5'] = group['monotonic_increasing_pct_21'].pct_change().rolling(5).sum()\n",
    "        group['monotonic_decreasing_pct_21_accel_5'] = group['monotonic_decreasing_pct_21'].pct_change().rolling(5).sum()\n",
    "\n",
    "        return group\n",
    "\n",
    "    @staticmethod\n",
    "    def fix_df(df_input: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert a DataFrame or Series to a standard (date, signal) MultiIndex with sorted index.\n",
    "\n",
    "        Args:\n",
    "            df_input (pd.DataFrame or pd.Series): Input data.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Reindexed/sorted DataFrame with MultiIndex (date, signal).\n",
    "        \"\"\"\n",
    "        df_input = df_input.copy()\n",
    "        if isinstance(df_input, pd.Series):\n",
    "            col_name = df_input.name\n",
    "            df_input = df_input.reset_index()\n",
    "            df_input.columns = ['date', 'signal', col_name]\n",
    "        else:\n",
    "            cols = df_input.columns.tolist()\n",
    "            df_input = df_input.reset_index()\n",
    "            df_input.columns = ['date', 'signal'] + cols\n",
    "\n",
    "        df_input.drop_duplicates(['date', 'signal'], inplace=True)\n",
    "        df_input = df_input.set_index(['date', 'signal']).sort_index()\n",
    "        return df_input\n",
    "\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"\n",
    "    Class responsible for defining and training models in a rolling-window fashion\n",
    "    and returning the final predictions DataFrame.\n",
    "\n",
    "    By default, uses LightGBM. If use_catboost=True, uses CatBoost instead.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 lookback: int = 252, \n",
    "                 forecast_horizon: int = 5,\n",
    "                 target_col: str = 'target',\n",
    "                 use_catboost: bool = False,\n",
    "                 catboost_params: dict = None):\n",
    "        \"\"\"\n",
    "        Initialize the ModelTrainer.\n",
    "\n",
    "        Args:\n",
    "            lookback (int): Number of days for rolling training window.\n",
    "            forecast_horizon (int): Horizon used to define the target window.\n",
    "            target_col (str): The name of the target column.\n",
    "            use_catboost (bool): Whether to use CatBoost instead of LightGBM.\n",
    "            catboost_params (dict): Parameters to pass to CatBoost if use_catboost is True.\n",
    "        \"\"\"\n",
    "        self.lookback = lookback\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.target_col = target_col\n",
    "        self.cpu_count = 2  # or cpu_count(), your choice\n",
    "\n",
    "        self.use_catboost = use_catboost\n",
    "        self.catboost_params = catboost_params or {}\n",
    "\n",
    "    def _process_signal(self, \n",
    "                        signal: str, \n",
    "                        group: pd.DataFrame, \n",
    "                        target_series: pd.Series) -> tuple:\n",
    "        \"\"\"\n",
    "        For a given signal, train a model (LightGBM or CatBoost) using a rolling window approach\n",
    "        and produce predictions (and feature importances if desired).\n",
    "\n",
    "        Args:\n",
    "            signal (str): The signal identifier.\n",
    "            group (pd.DataFrame): Predictors for this signal over time (indexed by date).\n",
    "            target_series (pd.Series): Target data for this signal over time (indexed by date).\n",
    "\n",
    "        Returns:\n",
    "            tuple: (predictions_list, importances_list)\n",
    "                   - predictions_list: list of dicts with 'date', 'signal', 'predicted_target', 'actual_target'\n",
    "                   - importances_list: list of feature importance dicts\n",
    "        \"\"\"\n",
    "        predictions_signal = []\n",
    "        importances_signal = []\n",
    "\n",
    "        # Ensure sorted by date\n",
    "        group = group.sort_index(level='date')\n",
    "        target_series = target_series.sort_index(level='date')\n",
    "        n = len(group)\n",
    "\n",
    "        # Not enough data to do rolling\n",
    "        if n < self.lookback + self.forecast_horizon:\n",
    "            return predictions_signal, importances_signal\n",
    "\n",
    "        # Lazy import CatBoost if actually needed\n",
    "        if self.use_catboost:\n",
    "            from catboost import CatBoostRegressor\n",
    "\n",
    "        for t in range(self.lookback + self.forecast_horizon, n - self.forecast_horizon):\n",
    "            # Training set\n",
    "            X_train = group.drop(columns=[self.target_col]).iloc[t - self.lookback - self.forecast_horizon : t - self.forecast_horizon].copy()\n",
    "            y_train = target_series.iloc[t - self.lookback - self.forecast_horizon : t - self.forecast_horizon].copy()\n",
    "\n",
    "            # Test set (single row at time t)\n",
    "            X_test = group.drop(columns=[self.target_col]).iloc[t : t+1].copy()\n",
    "\n",
    "            # Choose model\n",
    "            if self.use_catboost:\n",
    "                model = CatBoostRegressor(**self.catboost_params)\n",
    "                # For regression tasks, we often just call fit without additional params\n",
    "                # If you need cat_features, text_features, etc., you must specify them.\n",
    "                model.fit(X_train, y_train, verbose=0)\n",
    "            else:\n",
    "                model = lgb.LGBMRegressor(random_state=42, verbosity=-1, n_jobs=1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_test)[0]\n",
    "            actual_value = target_series.iloc[t]\n",
    "            pred_date = group.index[t]\n",
    "\n",
    "            predictions_signal.append({\n",
    "                'date': pred_date,\n",
    "                'signal': signal,\n",
    "                'predicted_target': y_pred,\n",
    "                'actual_target': actual_value\n",
    "            })\n",
    "\n",
    "            # Feature importances\n",
    "            if self.use_catboost:\n",
    "                # CatBoost has feature_importances_ but it's obtained differently\n",
    "                importance_values = model.get_feature_importance()\n",
    "                importance_dict = dict(zip(X_train.columns, importance_values))\n",
    "            else:\n",
    "                importance_dict = dict(zip(X_train.columns, model.feature_importances_))\n",
    "            importances_signal.append(importance_dict)\n",
    "\n",
    "        return predictions_signal, importances_signal\n",
    "\n",
    "    def train_and_predict(self, df_model: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Orchestrate rolling-window training for all signals in parallel and build\n",
    "        a final predictions DataFrame.\n",
    "\n",
    "        Args:\n",
    "            df_model (pd.DataFrame): Contains both features and target, \n",
    "                                     indexed by (date, signal).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Predictions with columns ['predicted_target', 'actual_target'] \n",
    "                          and MultiIndex (date, signal).\n",
    "        \"\"\"\n",
    "        # Drop rows without a target\n",
    "        df_model = df_model.dropna(subset=[self.target_col]).copy()\n",
    "\n",
    "        # Parallel over signals\n",
    "        grouped = df_model.groupby(level='signal')\n",
    "        results = []\n",
    "        with tqdm_joblib(tqdm(desc=\"Processing Signals\", total=len(grouped))):\n",
    "            results = Parallel(n_jobs=self.cpu_count)(\n",
    "                delayed(self._process_signal)(\n",
    "                    signal,\n",
    "                    group,\n",
    "                    df_model.xs(signal, level='signal')[self.target_col]\n",
    "                )\n",
    "                for signal, group in grouped\n",
    "            )\n",
    "\n",
    "        # Flatten the results\n",
    "        all_predictions = [pred for res in results for pred in res[0]]\n",
    "        all_importances = [imp for res in results for imp in res[1]]  # if you want to use them\n",
    "\n",
    "        df_predictions2 = pd.DataFrame(all_predictions)\n",
    "\n",
    "        # Clean \"date\" column in case it's a tuple\n",
    "        df_predictions2['date'] = df_predictions2['date'].apply(\n",
    "            lambda x: x[0] if isinstance(x, tuple) else x\n",
    "        )\n",
    "\n",
    "        # If \"actual_target\" is still a Series, fix it\n",
    "        df_predictions2['actual_target'] = df_predictions2['actual_target'].apply(\n",
    "            lambda x: x.values[0] if isinstance(x, pd.Series) else x\n",
    "        )\n",
    "\n",
    "        # Convert \"date\" to datetime\n",
    "        df_predictions2['date'] = pd.to_datetime(df_predictions2['date'])\n",
    "\n",
    "        # Create MultiIndex\n",
    "        df_predictions2.set_index(['date', 'signal'], inplace=True)\n",
    "\n",
    "        return df_predictions2\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    An end-to-end pipeline that:\n",
    "      1) Fetches raw data from BigQuery,\n",
    "      2) Engineers features and targets,\n",
    "      3) Trains the model in a rolling fashion,\n",
    "      4) Returns the final predictions DataFrame (df_predictions2).\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 project_id='issachar-feature-library', \n",
    "                 dataset_name='wmg',\n",
    "                 invalid_signals=None,\n",
    "                 use_catboost=False,\n",
    "                 catboost_params=None):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "\n",
    "        Args:\n",
    "            project_id (str): Google Cloud project ID.\n",
    "            dataset_name (str): BigQuery dataset name.\n",
    "            invalid_signals (list): A list of signals to exclude, if any.\n",
    "            use_catboost (bool): Whether to use CatBoost (True) or LightGBM (False, default).\n",
    "            catboost_params (dict): Parameters for CatBoost if use_catboost=True.\n",
    "        \"\"\"\n",
    "        self.project_id = project_id\n",
    "        self.dataset_name = dataset_name\n",
    "        self.invalid_signals = invalid_signals if invalid_signals else []\n",
    "        \n",
    "        self.data_fetcher = DataFetcher(project_id=project_id, \n",
    "                                        dataset_name=dataset_name)\n",
    "        self.feature_engineer = FeatureEngineer()\n",
    "\n",
    "        # We will instantiate ModelTrainer in run_pipeline()\n",
    "        self.model_trainer = None  \n",
    "\n",
    "        # Placeholders\n",
    "        self.all_signals = []\n",
    "        self.df_ic_daily = None\n",
    "        self.df_fvalues = None\n",
    "        self.df_mi_rolling = None\n",
    "        self.df_mi_daily = None\n",
    "        self.df_mono = None\n",
    "        self.df_combined = None\n",
    "        self.df_features_enhanced = None\n",
    "        self.df_target = None\n",
    "\n",
    "        self.use_catboost = use_catboost\n",
    "        self.catboost_params = catboost_params or {}\n",
    "\n",
    "    def run_pipeline(self,\n",
    "                     table_ic_daily='daily_ic2',\n",
    "                     table_fvalues='daily_fvalue_interactions',\n",
    "                     table_mi_rolling='mi_rolling',\n",
    "                     table_mi_daily='mi_daily',\n",
    "                     table_monotonicity='daily_rolling_monotonic_pct_st',\n",
    "                     target_duration='regular',  # or 'rolling'\n",
    "                     target_abs=True):\n",
    "        \"\"\"\n",
    "        Execute the entire pipeline, from data fetch to final predictions. \n",
    "        Returns df_predictions2.\n",
    "\n",
    "        Args:\n",
    "            table_ic_daily (str): Table name for daily IC data.\n",
    "            table_fvalues (str): Table name for f-values data.\n",
    "            table_mi_rolling (str): Table name for rolling MI data.\n",
    "            table_mi_daily (str): Table name for daily MI data.\n",
    "            table_monotonicity (str): Table name for daily rolling monotonicity data.\n",
    "            target_duration (str): 'regular' or 'rolling' target.\n",
    "            target_abs (bool): Whether to use absolute IC for target in rolling mode.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The final predictions DataFrame (df_predictions2).\n",
    "        \"\"\"\n",
    "        # 1. Fetch signals and filter them\n",
    "        all_signals_raw = self.data_fetcher.get_unique_signals_from_daily_ic(table_ic_daily)\n",
    "        self.all_signals = [s for s in all_signals_raw if s not in self.invalid_signals]\n",
    "\n",
    "        # 2. Fetch daily IC data\n",
    "        self.df_ic_daily = self.data_fetcher.fetch_data_from_gbq(table_ic_daily)\n",
    "        self.df_ic_daily = self.df_ic_daily[~self.df_ic_daily.index.duplicated(keep='first')]\n",
    "\n",
    "        # Compute 21-day rolling\n",
    "        self.df_ic_daily = self.feature_engineer.compute_rolling_ic(self.df_ic_daily)\n",
    "\n",
    "        # 3. Fetch f-values, mi_rolling, mi_daily, monotonicity\n",
    "        self.df_fvalues = self.data_fetcher.fetch_data_from_gbq_fvalues(table_fvalues, self.all_signals)\n",
    "        self.df_fvalues = self.df_fvalues[~self.df_fvalues.index.duplicated(keep='first')]\n",
    "        \n",
    "        self.df_mi_rolling = self.data_fetcher.fetch_data_from_gbq(table_mi_rolling)\n",
    "        self.df_mi_rolling = self.df_mi_rolling[~self.df_mi_rolling.index.duplicated(keep='first')]\n",
    "\n",
    "        self.df_mi_daily = self.data_fetcher.fetch_data_from_gbq(table_mi_daily)\n",
    "        self.df_mi_daily = self.df_mi_daily[~self.df_mi_daily.index.duplicated(keep='first')]\n",
    "\n",
    "        self.df_mono = self.data_fetcher.fetch_data_from_gbq(table_monotonicity)\n",
    "        self.df_mono = self.df_mono[~self.df_mono.index.duplicated(keep='first')]\n",
    "        self.df_mono['monotonic_21_diff'] = (\n",
    "            self.df_mono['monotonic_increasing_pct_21'] - \n",
    "            self.df_mono['monotonic_decreasing_pct_21']\n",
    "        )\n",
    "\n",
    "        # 4. Combine everything\n",
    "        self.df_combined = self.df_ic_daily \\\n",
    "            .join(self.df_fvalues) \\\n",
    "            .join(self.df_mi_rolling) \\\n",
    "            .join(self.df_mi_daily) \\\n",
    "            .join(self.df_mono) \\\n",
    "            .copy()\n",
    "\n",
    "        # 5. Engineer rolling features on combined data\n",
    "        self.df_features_enhanced = self.df_combined.groupby(level='signal') \\\n",
    "            .apply(self.feature_engineer.compute_rolling_features).copy()\n",
    "        # The grouping above can shift the signal level up, so drop it\n",
    "        self.df_features_enhanced.index = self.df_features_enhanced.index.droplevel(0)\n",
    "\n",
    "        # 6. Define the target\n",
    "        if target_duration == 'regular':\n",
    "            self.df_target = self.df_combined['all_IC_spearmanr'].copy()\n",
    "        elif target_duration == 'rolling':\n",
    "            roll = 5\n",
    "            shift_n = 4\n",
    "            if not target_abs:\n",
    "                # Rolling std of future 5 days\n",
    "                df_forward_5day_std_ic = self.df_combined.groupby(level='signal')['all_IC_spearmanr'] \\\n",
    "                    .apply(lambda x: x.shift(-1).rolling(window=roll).std().shift(-shift_n))\n",
    "                df_forward_5day_std_ic.index = df_forward_5day_std_ic.index.droplevel(0)\n",
    "                df_forward_5day_std_ic.name = 'target'\n",
    "                self.df_target = df_forward_5day_std_ic.copy()\n",
    "            else:\n",
    "                # Rolling std of future 5 days of abs(IC)\n",
    "                df_forward_5day_std_ic_abs = self.df_combined.groupby(level='signal')['all_IC_spearmanr'] \\\n",
    "                    .apply(lambda x: x.shift(-1).abs().rolling(window=roll).std().shift(-shift_n))\n",
    "                df_forward_5day_std_ic_abs.index = df_forward_5day_std_ic_abs.index.droplevel(0)\n",
    "                df_forward_5day_std_ic_abs.name = 'target'\n",
    "                self.df_target = df_forward_5day_std_ic_abs.copy()\n",
    "\n",
    "        # Standardize target name\n",
    "        self.df_target.name = 'target'\n",
    "\n",
    "        # 7. Shift features by 2 days to avoid lookahead\n",
    "        df_features_enhanced_shifted = self.df_features_enhanced.groupby(level='signal').shift(2).copy()\n",
    "        df_features_enhanced_shifted = FeatureEngineer.fix_df(df_features_enhanced_shifted)\n",
    "\n",
    "        # 8. Combine shifted features + target\n",
    "        df_model = pd.concat([df_features_enhanced_shifted, \n",
    "                              FeatureEngineer.fix_df(self.df_target)], \n",
    "                             axis=1)\n",
    "\n",
    "        # 9. Train the model in a rolling-window and return predictions\n",
    "        self.model_trainer = ModelTrainer(\n",
    "            lookback=252, \n",
    "            forecast_horizon=5, \n",
    "            target_col='target',\n",
    "            use_catboost=self.use_catboost,\n",
    "            catboost_params=self.catboost_params\n",
    "        )\n",
    "        df_predictions2 = self.model_trainer.train_and_predict(df_model)\n",
    "\n",
    "        return df_predictions2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58467d76-6c79-433a-84da-9256812cfe1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8c447d-136a-4b5d-ac13-5c6e70077a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "catboost = False\n",
    "\n",
    "if not catboost:\n",
    "    # Example usage:\n",
    "\n",
    "    # By default, this uses LightGBM:\n",
    "    pipeline_lgb = Pipeline(\n",
    "        project_id='issachar-feature-library', \n",
    "        dataset_name='wmg',\n",
    "        invalid_signals=[],  # or pass a list of signals to exclude\n",
    "        use_catboost=False   # <--- LightGBM is the default\n",
    "    )\n",
    "\n",
    "    df_predictions2_lgb = pipeline_lgb.run_pipeline(\n",
    "        table_ic_daily='daily_ic2',\n",
    "        table_fvalues='daily_fvalue_interactions',\n",
    "        table_mi_rolling='mi_rolling',\n",
    "        table_mi_daily='mi_daily',\n",
    "        table_monotonicity='daily_rolling_monotonic_pct_st',\n",
    "        target_duration='regular',   # or 'rolling'\n",
    "        target_abs=False\n",
    "    )\n",
    "\n",
    "#     df_predictions2_lgb.to_gbq(\n",
    "#         destination_table='wmg.ic_predictions_v2_lgb', \n",
    "#         project_id='issachar-feature-library', \n",
    "#         if_exists='replace'\n",
    "#     )\n",
    "#     print(\"Predictions uploaded to BigQuery table 'wmg.ic_predictions_v2' successfully!\")\n",
    "\n",
    "else:\n",
    "\n",
    "    # Example: Use CatBoost instead:\n",
    "    pipeline_cb = Pipeline(\n",
    "        project_id='issachar-feature-library', \n",
    "        dataset_name='wmg',\n",
    "        invalid_signals=[], \n",
    "        use_catboost=True,\n",
    "        catboost_params=catboost_params  # your CatBoost parameter dict\n",
    "    )\n",
    "\n",
    "    df_predictions2_cb = pipeline_cb.run_pipeline(\n",
    "        table_ic_daily='daily_ic2',\n",
    "        table_fvalues='daily_fvalue_interactions',\n",
    "        table_mi_rolling='mi_rolling',\n",
    "        table_mi_daily='mi_daily',\n",
    "        table_monotonicity='daily_rolling_monotonic_pct_st',\n",
    "        target_duration='regular',\n",
    "        target_abs=False\n",
    "    )\n",
    "\n",
    "#     df_predictions2_cb.to_gbq(\n",
    "#         destination_table='wmg.ic_predictions_v2_catboost', \n",
    "#         project_id='issachar-feature-library', \n",
    "#         if_exists='replace'\n",
    "#     )\n",
    "#     print(\"CatBoost Predictions uploaded to BigQuery table 'wmg.ic_predictions_v2_catboost' successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
